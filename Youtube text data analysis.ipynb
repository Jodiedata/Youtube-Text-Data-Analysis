{"cells":[{"cell_type":"markdown","source":["### Youtube comments analysis\nIn this notebook, we have a dataset of user comments for youtube videos related to animals or pets. We will attempt to identify cat or dog owners based on these comments, find out the topics important to them, and then identify video creators with the most viewers that are cat or dog owners."],"metadata":{}},{"cell_type":"markdown","source":["The dataset provided for this coding test are comments for videos related to animals and/or pets. The dataset is 240MB compressed\nhttps://drive.google.com/file/d/1o3DsS3jN_t2Mw3TsV0i7ySRmh9kyYi1a/view?usp=sharing\n\n The dataset file is comma separated, with a header line defining the field names, listed here: <br>\n‚óè creator_name. Name of the YouTube channel creator.<br>\n‚óè userid. Integer identifier for the users commenting on the YouTube channels.<br>\n‚óè comment. Text of the comments made by the users."],"metadata":{}},{"cell_type":"markdown","source":["#### Download the data"],"metadata":{}},{"cell_type":"code","source":["# link: https://drive.google.com/file/d/1o3DsS3jN_t2Mw3TsV0i7ySRmh9kyYi1a/view?usp=sharing\n!pip install googledrivedownloader\nfrom google_drive_downloader import GoogleDriveDownloader as gdd\ngdd.download_file_from_google_drive(file_id='1o3DsS3jN_t2Mw3TsV0i7ySRmh9kyYi1a', dest_path='./../../dbfs/Youtube/data/animal_comments.gz')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["# 0. Data Exploration and Cleaning"],"metadata":{}},{"cell_type":"code","source":["# read data\ndf = spark.read.load('/Youtube/data/animal_comments.gz', format='csv', header = True, inferSchema = True)\ndf.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+------+-------------------------------------+\n        creator_name|userid|                              comment|\n+--------------------+------+-------------------------------------+\n        Doug The Pug|  87.0|                 I shared this to ...|\n        Doug The Pug|  87.0|                   Super cute  üòÄüêïüê∂|\n         bulletproof| 530.0|                 stop saying get e...|\n       Meu Zool√≥gico| 670.0|                 Tenho uma jiboia ...|\n              ojatro|1031.0|                 I wanna see what ...|\n     Tingle Triggers|1212.0|                 Well shit now Im ...|\nHope For Paws - O...|1806.0|                 when I saw the en...|\nHope For Paws - O...|2036.0|                 Holy crap. That i...|\n          Life Story|2637.0|Ê≠¶Âô®„ÅØ„ÇØ„Ç®„Çπ„Éà„ÅßË≤∞„Åà„Çã„Çì„Åò„ÇÉ„Å™„ÅÑ„Çì...|\n       Brian Barczyk|2698.0|                 Call the teddy Larry|\n+--------------------+------+-------------------------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["df.count() "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: 5820035</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["####Check missing values"],"metadata":{}},{"cell_type":"code","source":["# Count null values in each columns \nprint('Number of null values in creator_name: ',df.filter(df['creator_name'].isNull()).count())\nprint('Number of null values in userid: ',df.filter(df['userid'].isNull()).count())\nprint('Number of null values in comment: ',df.filter(df['comment'].isNull()).count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of null values in creator_name:  32050\nNumber of null values in userid:  565\nNumber of null values in comment:  1051\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["####Drop data with no comments"],"metadata":{}},{"cell_type":"code","source":["df = df.na.drop(subset=[\"comment\"])\nprint('Number of rows after droping null comment:',df.count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of rows after droping null comment: 5818984\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["####Convert comment text to lower case"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as F\ndf_clean = df.withColumn('comment', F.lower(F.col('comment')))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["####Data overview"],"metadata":{}},{"cell_type":"code","source":["df_clean.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+------+-------------------------------------+\n        creator_name|userid|                              comment|\n+--------------------+------+-------------------------------------+\n        Doug The Pug|  87.0|                 i shared this to ...|\n        Doug The Pug|  87.0|                   super cute  üòÄüêïüê∂|\n         bulletproof| 530.0|                 stop saying get e...|\n       Meu Zool√≥gico| 670.0|                 tenho uma jiboia ...|\n              ojatro|1031.0|                 i wanna see what ...|\n     Tingle Triggers|1212.0|                 well shit now im ...|\nHope For Paws - O...|1806.0|                 when i saw the en...|\nHope For Paws - O...|2036.0|                 holy crap. that i...|\n          Life Story|2637.0|Ê≠¶Âô®„ÅØ„ÇØ„Ç®„Çπ„Éà„ÅßË≤∞„Åà„Çã„Çì„Åò„ÇÉ„Å™„ÅÑ„Çì...|\n       Brian Barczyk|2698.0|                 call the teddy larry|\n            The Dodo|2702.0|                   üòêü§îüòìüò¢üò≠üò≠üò≠üò≠üòü|\nHope For Paws - O...|2911.0|                 that mother cat l...|\nHope For Paws - O...|2911.0|                 its people like h...|\n   Talking Kitty Cat|2911.0|                 steve: no wet foo...|\n    Brave Wilderness|3224.0|                 dont call this a ...|\n          MaxluvsMya|3267.0|                 why are you alway...|\nRise Up Society F...|3372.0|                           deb tucker|\n            The Dodo|3466.0|                 thats a deer isnt...|\n    Brave Wilderness|3466.0|                 there is no safe ...|\n    Brave Wilderness|3466.0|                 red before yellow...|\n+--------------------+------+-------------------------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["#Part 1. Data preprocessing"],"metadata":{}},{"cell_type":"markdown","source":["### Tokenize the text data and create word2Vec features"],"metadata":{}},{"cell_type":"markdown","source":["Select 2MM data for this project to use."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import rand \n\ndf_clean.orderBy(rand(seed=0)).createOrReplaceTempView(\"table\")\ndf_clean = spark.sql(\"select * from table limit 2000000\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer, Word2Vec\nfrom pyspark.ml.classification import LogisticRegression\n\n# regular expression tokenizer\nregexTokenizer = RegexTokenizer(inputCol=\"comment\", outputCol=\"words\", pattern=\"\\\\W\")\nword2Vec = Word2Vec(inputCol=\"words\", outputCol=\"features\") #the tokenized feature captures semantic similarity of that word to other words"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=[regexTokenizer, word2Vec])\n\n# Fit the pipeline to training documents.\npipelineFit = pipeline.fit(df_clean)\ndataset = pipelineFit.transform(df_clean)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["dataset.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------+--------------------+--------------------+--------------------+\n        creator_name|   userid|             comment|               words|            features|\n+--------------------+---------+--------------------+--------------------+--------------------+\n     Gohan The Husky| 106460.0|keep him! and cal...|[keep, him, and, ...|[-0.0242814878718...|\n          Real Shock|1050336.0|—è –Ω–∞ –≤—Å–µ –ø—Ä–∞–≤–∏–ª—å–Ω...|                  []|         (100,[],[])|\n       Brian Barczyk| 636384.0| hogwarts üêçüêçüêçüêçüêç|          [hogwarts]|[0.11475221812725...|\n  Think Like A Horse|1411449.0|i love how you al...|[i, love, how, yo...|[0.06695287060990...|\n            M·∫°nh CFM| 879606.0|      hai vai a manh| [hai, vai, a, manh]|[-0.0317196808755...|\n Home Aquatics Hobby|1995671.0|      imma try this!|   [imma, try, this]|[0.15546574940284...|\n             Sad Cat|2039444.0|the 2nd dog was t...|[the, 2nd, dog, w...|[0.12009738194935...|\nHope For Paws - O...| 565118.0|my eyes on tears ...|[my, eyes, on, te...|[0.04250455430398...|\n         FROSTY Life| 892104.0|nature protection...|[nature, protecti...|[0.01910656044880...|\n            ViralHog|2270703.0|      damn she thick|  [damn, she, thick]|[0.03960126638412...|\n+--------------------+---------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["###Selecting 1000000 data to build the classifier"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import rand \n\ndataset.orderBy(rand(seed=0)).createOrReplaceTempView(\"table\")\nsampled = spark.sql(\"select * from table limit 1000000\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["#### Label the data\nThis is an unlabeled dataset and we want to train a clasifier to identify cat and dog owners. Thus first thing to do is to label each comment. <br>\nLabel comment when he/she has dogs or cats. <br>\nlabel comment when he/she don't have a dog or cat. <br>\nCombine 1 and 2 as our training dataset, and rest of the dataset will be the data we predict. <br>"],"metadata":{}},{"cell_type":"code","source":["# find user with preference of dog and cat\nfrom pyspark.sql.functions import when\nfrom pyspark.sql.functions import col\n\nsampled = sampled.withColumn(\"label\", \\\n                           (when(col(\"comment\").like(\"%my dog%\"), 1) \\\n                            .when(col(\"comment\").like(\"%my dogs%\"), 1) \\\n                           .when(col(\"comment\").like(\"%i have a dog%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my cat%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my cats%\"), 1) \\\n                           .when(col(\"comment\").like(\"%i have a cat%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my puppy%\"), 1) \\\n                           .when(col(\"comment\").like(\"%i have a puppy%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my pup%\"), 1) \\\n                            .when(col(\"comment\").like(\"%i have a pup%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my kitty%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my kitten%\"), 1) \\\n                           .when(col(\"comment\").like(\"%my pussy%\"), 1) \\\n                           .when(col(\"comment\").like(\"i have a kitty\"),1)\\\n                           .when(col(\"comment\").like(\"i have a kitten\"),1)\\\n                           .when(col(\"comment\").like(\"my own\"),1)\\\n                           .otherwise(0)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["sampled.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+---------+--------------------+--------------------+--------------------+-----+\n       creator_name|   userid|             comment|               words|            features|label|\n+-------------------+---------+--------------------+--------------------+--------------------+-----+\n   Brave Wilderness| 254809.0|for as much as i ...|[for, as, much, a...|[-0.1172484595860...|    0|\n          –¢–û–ü –°–ê–ú–´–•|1652025.0|                   –ª|                  []|         (100,[],[])|    0|\n           HammyLux| 159772.0|ah! your petco is...|[ah, your, petco,...|[0.08070177207183...|    0|\n     LightningLpsTV|1881033.0|fun fact:hot dogs...|[fun, fact, hot, ...|[0.03406326798722...|    0|\n       Mr. Max T.V.|1936350.0|thanks max i am a...|[thanks, max, i, ...|[-0.0309817560094...|    0|\n Think Like A Horse|2007426.0|u r soon right. h...|[u, r, soon, righ...|[0.08446431501458...|    0|\n The Pet Collective|1382459.0|*i love how the t...|[i, love, how, th...|[0.01836268778424...|    0|\n      Viral Maniacs|1817340.0|–ª–∞–π–∫ –µ—Å–ª–∏ –∑–∞–≤–∞—Ä–∞–∂...|                  []|         (100,[],[])|    0|\nKeedes channel LIVE| 723319.0|15:16 –æ–∫ –∞ —Ç–æ —è —É...|            [15, 16]|[0.03535630740225...|    0|\n   Brave Wilderness|2092494.0|sillybilly thats ...|[sillybilly, that...|[-0.0838636914268...|    0|\n+-------------------+---------+--------------------+--------------------+--------------------+-----+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["### Downsampling the dataset"],"metadata":{}},{"cell_type":"markdown","source":["Note that number of negative labels is around 100 times more than positive labels, so here we need to downsampling the negative labels. By rule of thumb, the gap should be no more than 10 times. But here I make them balance to the ratio aroudn 1:2 (1 for positive: 2 for negative)"],"metadata":{}},{"cell_type":"code","source":["own_pets = sampled.filter(col('label')==1)\nno_pets = sampled.filter(col('label')==0)\nprint(\"Number of confirmed users who own dogs or cats in training data set: \", own_pets.count())\nprint(\"Number of confirmed users who don't have pet's in training data set: \", no_pets.count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of confirmed users who own dogs or cats in training data set:  9487\nNumber of confirmed users who don&#39;t have pet&#39;s in training data set:  990513\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["from pyspark.sql.functions import rand \nno_pets.orderBy(rand()).createOrReplaceTempView(\"table\")\n\nNum_Pos_Label = own_pets.count() \nNum_Neg_Label = no_pets.count()\n\ndf_no_pets_down = spark.sql(\"select * from table where limit {}\".format(Num_Pos_Label*2))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["print('Now after balancing the lables, we have ')   \nprint('Positive label: ', Num_Pos_Label)\nprint('Negtive label: ', df_no_pets_down.count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Now after balancing the lables, we have \nPositive label:  9487\nNegtive label:  18974\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["df_model = own_pets.union(df_no_pets_down)\ndf_model.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------+--------------------+--------------------+--------------------+-----+\n        creator_name|   userid|             comment|               words|            features|label|\n+--------------------+---------+--------------------+--------------------+--------------------+-----+\nDog Training by K...|  14593.0|i have a chow. iv...|[i, have, a, chow...|[0.07402610394065...|    1|\n      Big Cat Rescue|1970394.0|if my cat sounds ...|[if, my, cat, sou...|[0.08496134922218...|    1|\n           meow meow|1704771.0|my cats are going...|[my, cats, are, g...|[0.07563407280083...|    1|\n            The Dodo|1727704.0|my cat would not ...|[my, cat, would, ...|[0.11237937398254...|    1|\nZak Georges Dog T...|1217044.0|ok so i love your...|[ok, so, i, love,...|[0.05536452893766...|    1|\nHope For Paws - O...|1585600.0|should have calle...|[should, have, ca...|[-0.0310175169089...|    1|\n           Vet Ranch|1412999.0|omg...i want her....|[omg, i, want, he...|[0.04523689354149...|    1|\nZak Georges Dog T...|1813543.0|my dog is not rea...|[my, dog, is, not...|[0.07030598035524...|    1|\n        Robin Seplut|1521289.0|thanks for the in...|[thanks, for, the...|[0.03279427839443...|    1|\n       Brian Barczyk| 869161.0|plz do not cry i ...|[plz, do, not, cr...|[0.02789124846458...|    1|\n+--------------------+---------+--------------------+--------------------+--------------------+-----+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["#Part 2. Model Training and Evaluation"],"metadata":{}},{"cell_type":"code","source":["train, test = df_model.randomSplit([0.8, 0.2], seed=12345)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["###Logistic Regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n\nlr = LogisticRegression(featuresCol=\"features\",labelCol=\"label\" , maxIter=10, regParam=0.1, elasticNetParam=0.8)\n\n# Run TrainValidationSplit, and choose the best set of parameters.\nlrModel = lr.fit(train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":37},{"cell_type":"code","source":["summary_lr = lrModel.summary\n# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nroc = summary_lr.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.plot([0, 1], [0, 1], 'k--')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve')\nplt.show()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["print(\"areaUnderROC: \" + str(summary_lr.areaUnderROC))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">areaUnderROC: 0.8849961224029992\n</div>"]}}],"execution_count":39},{"cell_type":"code","source":["# Set the model threshold to maximize F-Measure\nfMeasure = summary_lr .fMeasureByThreshold\nmaxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\nbestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n                .select('threshold').head()['threshold']\n# Make predictions on test data. model is the model with combination of parameters\n# that performed best.\npredictions_lr = lrModel.transform(test,{lrModel.threshold: bestThreshold})\npredictions_lr.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n  creator_name|   userid|             comment|               words|            features|label|       rawPrediction|         probability|prediction|\n+--------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n          null| 939558.0|if i saw my cat f...|[if, i, saw, my, ...|[0.05614113026800...|    1|[0.4870847760543,...|[0.61941944002529...|       1.0|\n          null|1265524.0|my cat would slau...|[my, cat, would, ...|[0.03301996208028...|    1|[0.01956351767371...|[0.50489072343337...|       1.0|\n          null|1591257.0|notice the tiger ...|[notice, the, tig...|[0.10342899509878...|    1|[0.49246764454753...|[0.62068757380402...|       1.0|\n    101rabbits| 946174.0|i used to have an...|[i, used, to, hav...|[0.01570107034907...|    1|[0.48477938070999...|[0.61887581901535...|       1.0|\n    101rabbits|1465076.0|mocha has the sam...|[mocha, has, the,...|[-0.0828659053659...|    1|[-0.1071429124634...|[0.47323986663140...|       1.0|\n    101rabbits|2069730.0|omg your bunnys a...|[omg, your, bunny...|[0.04464152043219...|    1|[0.42127325559381...|[0.60378788856477...|       1.0|\nAarons Animals| 937383.0|this is like my c...|[this, is, like, ...|[-0.0294186978672...|    1|[0.49364340508288...|[0.62096434910277...|       1.0|\nAarons Animals|1345702.0|i didnt think you...|[i, didnt, think,...|[0.18048949539661...|    1|[0.45657514529346...|[0.61220139160613...|       1.0|\nAarons Animals|1483408.0|my cat also did m...|[my, cat, also, d...|[0.05328114432367...|    1|[0.38110299468359...|[0.59413910427085...|       1.0|\nAarons Animals|2032829.0|my cat just died ...|[my, cat, just, d...|[-0.0069224577488...|    1|[0.42237302555556...|[0.60405095436119...|       1.0|\n+--------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":40},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n\ndef get_evaluation_result(predictions):\n  evaluator = BinaryClassificationEvaluator(\n      labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n  AUC = evaluator.evaluate(predictions)\n\n  TP = predictions[(predictions[\"label\"] == 1) & (predictions[\"prediction\"] == 1.0)].count()\n  FP = predictions[(predictions[\"label\"] == 0) & (predictions[\"prediction\"] == 1.0)].count()\n  TN = predictions[(predictions[\"label\"] == 0) & (predictions[\"prediction\"] == 0.0)].count()\n  FN = predictions[(predictions[\"label\"] == 1) & (predictions[\"prediction\"] == 0.0)].count()\n\n  accuracy = (TP + TN)*1.0 / (TP + FP + TN + FN)\n  precision = TP*1.0 / (TP + FP)\n  recall = TP*1.0 / (TP + FN)\n\n\n  print (\"True Positives:\", TP)\n  print (\"False Positives:\", FP)\n  print (\"True Negatives:\", TN)\n  print (\"False Negatives:\", FN)\n  print (\"Test Accuracy:\", accuracy)\n  print (\"Test Precision:\", precision)\n  print (\"Test Recall:\", recall)\n  print (\"Test AUC of ROC:\", AUC)\n\nprint(\"Prediction result summary for Logistic Regression Model:  \")\nget_evaluation_result(predictions_lr)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Prediction result summary for Logistic Regression Model:  \nTrue Positives: 1601\nFalse Positives: 858\nTrue Negatives: 3020\nFalse Negatives: 279\nTest Accuracy: 0.8025356026398055\nTest Precision: 0.651077673851159\nTest Recall: 0.8515957446808511\nTest AUC of ROC: 0.8771889445927803\n</div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["###Random Forest"],"metadata":{}},{"cell_type":"code","source":["\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Train a RandomForest model.\nrfModel = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=15)\n\n# Train model.  This also runs the indexers.\nmodel = rfModel.fit(train)\n\n# Make predictions.\npredictions_rf = model.transform(test)\n\n# Select example rows to display.\npredictions_rf.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n        creator_name|   userid|             comment|               words|            features|label|       rawPrediction|         probability|prediction|\n+--------------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n                null|1855339.0|i would protect m...|[i, would, protec...|[0.04566170562793...|    1|[4.33430724161909...|[0.28895381610793...|       1.0|\n          101rabbits| 367209.0|hi im moving and ...|[hi, im, moving, ...|[8.29553438557518...|    1|[3.79561809136782...|[0.25304120609118...|       1.0|\n          101rabbits|1018999.0|when i saw the na...|[when, i, saw, th...|[0.02316679899452...|    1|[5.91471206295764...|[0.39431413753050...|       1.0|\n2CAN.TV - Ripley ...|1384044.0|you put a banana ...|[you, put, a, ban...|[0.08089200304239...|    1|[11.8407586516321...|[0.78938391010880...|       0.0|\n2CAN.TV - Ripley ...|2382136.0|do you spend a lo...|[do, you, spend, ...|[0.12231800931951...|    1|[6.31245551337581...|[0.42083036755838...|       1.0|\n 76Highboy Reloading|2462842.0|my dog is a repub...|[my, dog, is, a, ...|[0.03120583518304...|    1|[8.65896404212307...|[0.57726426947487...|       0.0|\n      Aarons Animals|2032829.0|my cat just died ...|[my, cat, just, d...|[-0.0069224577488...|    1|[4.67274230288730...|[0.31151615352582...|       1.0|\n      Aarons Animals|2381933.0|tried putting a g...|[tried, putting, ...|[0.08143107344706...|    1|[5.20213777417998...|[0.34680918494533...|       1.0|\nAbandoned NorthJe...| 171330.0|awww so cute look...|[awww, so, cute, ...|[0.01436597201973...|    1|[8.02853183445600...|[0.53523545563040...|       0.0|\n  Alex Knappenberger| 430815.0|  my dog is the same|[my, dog, is, the...|[-0.1069111421704...|    1|[4.08098716593775...|[0.27206581106251...|       1.0|\n+--------------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":43},{"cell_type":"code","source":["print(\"Prediction result summary for Random Forest Model:  \")\nget_evaluation_result(predictions_rf)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Prediction result summary for Random Forest Model:  \nTrue Positives: 1296\nFalse Positives: 168\nTrue Negatives: 3659\nFalse Negatives: 619\nTest Accuracy: 0.8629397422500871\nTest Precision: 0.8852459016393442\nTest Recall: 0.6767624020887728\nTest AUC of ROC: 0.9416125621576833\n</div>"]}}],"execution_count":44},{"cell_type":"markdown","source":["###Gradient Boosted"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Train a GDBT model.\ngbtModel = GBTClassifier(featuresCol=\"features\",labelCol=\"label\", maxIter=10)\n\n# Train model.  This also runs the indexer.\nmodel = gbtModel.fit(train)\n\n# Make predictions.\npredictions_gbt = model.transform(test)\n\n# Select example rows to display.\npredictions_gbt.show(10)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n       creator_name|   userid|             comment|               words|            features|label|       rawPrediction|         probability|prediction|\n+-------------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n               null|1859003.0|that guy who was ...|[that, guy, who, ...|[0.11277555616661...|    1|[-1.0628234487364...|[0.10662895531310...|       1.0|\n         101rabbits| 367209.0|hi im moving and ...|[hi, im, moving, ...|[8.29553438557518...|    1|[-1.0001455315652...|[0.11917236564742...|       1.0|\n         101rabbits| 946174.0|i used to have an...|[i, used, to, hav...|[0.01570107034907...|    1|[-1.0578667860207...|[0.10757698014139...|       1.0|\n         Aaron Rift|1217127.0|wish my cat did t...|[wish, my, cat, d...|[0.05897291228175...|    1|[-0.9495973448550...|[0.13019964674365...|       1.0|\n     Aarons Animals|  26421.0|best invasion eve...|[best, invasion, ...|[0.01281318785622...|    1|[-0.9382359811825...|[0.13279463704396...|       1.0|\n     Aarons Animals|  99588.0|your videos are s...|[your, videos, ar...|[0.03866720924162...|    1|[0.78163211631509...|[0.82682125024673...|       0.0|\n     Aarons Animals|1992132.0|my cat name is ma...|[my, cat, name, i...|[-0.0706151963211...|    1|[-0.0872436913765...|[0.45648849372193...|       1.0|\nAleiaAnimalLover368|1129471.0|im so sorry realy...|[im, so, sorry, r...|[0.02153120976355...|    1|[-0.8601617163336...|[0.15182950817778...|       1.0|\n Alex Knappenberger|  43143.0|if i had to guess...|[if, i, had, to, ...|[0.04421059365995...|    1|[-0.2955057156273...|[0.35640281867581...|       1.0|\n Alex Knappenberger| 695428.0|i hate this forci...|[i, hate, this, f...|[0.14996320175889...|    1|[-0.6290956747563...|[0.22128539844957...|       1.0|\n+-------------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":46},{"cell_type":"code","source":["print(\"Prediction result summary for Gradient Boosted Model:  \")\nget_evaluation_result(predictions_gbt)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Prediction result summary for Gradient Boosted Model:  \nTrue Positives: 1555\nFalse Positives: 362\nTrue Negatives: 3480\nFalse Negatives: 357\nTest Accuracy: 0.8750434480361488\nTest Precision: 0.8111632759520083\nTest Recall: 0.8132845188284519\nTest AUC of ROC: 0.9399335034226104\n</div>"]}}],"execution_count":47},{"cell_type":"markdown","source":["#Part 3. Hyperparamter Tuning"],"metadata":{}},{"cell_type":"markdown","source":["#### Tune hyperameters for GDBT model"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nimport numpy as np\n\n#Tune parameter using grid search and cross validation\nparamGrid_gbt = ParamGridBuilder()\\\n               .addGrid(rfModel.numTrees, [int(x) for x in np.linspace(start = 10, stop = 50, num = 3)]) \\\n               .addGrid(rfModel.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 25, num = 3)]) \\\n               .build()\n\nevaluator=BinaryClassificationEvaluator()\n\ncrossval_gbt = CrossValidator(\n               estimator=gbtModel,\n               estimatorParamMaps=paramGrid_gbt,\n               evaluator=evaluator,\n               numFolds=3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":50},{"cell_type":"markdown","source":["####Get the best model with the best parameters"],"metadata":{}},{"cell_type":"code","source":["#Best model with tuned parameters\ncvModel_gbt = crossval_gbt.fit(train)\n\n# Make predictions.\npredictions = cvModel_gbt.transform(test)\n\n# Select example rows to display.\npredictions.show(10)\n\nprint(\"Prediction result summary on test data for Gradient Boosted Model with tuned parameters:  \")\nget_evaluation_result(predictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n+--------------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n        creator_name|   userid|             comment|               words|            features|label|       rawPrediction|         probability|prediction|\n+--------------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n                null|1596933.0|my doggo freind m...|[my, doggo, frein...|[0.09965891477673...|    1|[0.82813834859425...|[0.83973756056947...|       0.0|\n                null|2016303.0|i cant have a bir...|[i, cant, have, a...|[0.11987633439194...|    1|[-0.6586928883918...|[0.21125356090386...|       1.0|\n                null|2381008.0|i see your snake ...|[i, see, your, sn...|[0.07449154711018...|    1|[-0.4705064751925...|[0.28069577674430...|       1.0|\n         1BubbaMike1| 531433.0|my cat thinks dat...|[my, cat, thinks,...|[0.02003000473932...|    1|[-1.0093993512012...|[0.11724326475488...|       1.0|\n          2 Pitbulls| 358681.0|my dog loves to s...|[my, dog, loves, ...|[0.07868712005967...|    1|[-0.4418116413799...|[0.29242750875345...|       1.0|\n2CAN.TV - Ripley ...| 662264.0|i can assure you ...|[i, can, assure, ...|[0.06884804462242...|    1|[-0.7905345086907...|[0.17064413636959...|       1.0|\n      AfroHerpkeeper| 839735.0|i might be gettin...|[i, might, be, ge...|[0.08326297533725...|    1|[-0.9223485334762...|[0.13649672519659...|       1.0|\n  Alex Knappenberger| 415413.0|you want to see a...|[you, want, to, s...|[0.15721849208189...|    1|[-0.7469945237502...|[0.18332374798336...|       1.0|\n  Alex Knappenberger| 442327.0|my dog has the si...|[my, dog, has, th...|[0.04860801990809...|    1|[-0.7271768059751...|[0.18933244502029...|       1.0|\n  Alex Knappenberger|1586986.0|my dog does that ...|[my, dog, does, t...|[-0.0051822586517...|    1|[-0.2425860659383...|[0.38103154766661...|       1.0|\n+--------------------+---------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 10 rows\n\nPrediction result summary on test data for Gradient Boosted Model with tuned parameters:  \nTrue Positives: 1599\nFalse Positives: 390\nTrue Negatives: 3434\nFalse Negatives: 341\nTest Accuracy: 0.8731783483691881\nTest Precision: 0.803921568627451\nTest Recall: 0.8242268041237113\nTest AUC of ROC: 0.947897404402701\n</div>"]}}],"execution_count":52},{"cell_type":"code","source":["#Fit the model to the full dataset and evaluate\npredictions_full = cvModel_gbt.transform(df_model)\nprint(\"Prediction result summary on full dataset for Gradient Boosted Model with tuned parameters:  \")\nget_evaluation_result(predictions_full)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Prediction result summary on full dataset for Gradient Boosted Model with tuned parameters:  \nTrue Positives: 7768\nFalse Positives: 1934\nTrue Negatives: 17040\nFalse Negatives: 1719\nTest Accuracy: 0.8716489230877341\nTest Precision: 0.8006596578025149\nTest Recall: 0.8188046800885422\nTest AUC of ROC: 0.9460317975025871\n</div>"]}}],"execution_count":53},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport itertools\n\n#Define a function print the confusion matraix\ndef plot_confusion_matrix(predictions,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    y_true =predictions.select(\"label\")\n    y_true = y_true.toPandas()\n    y_pred = predictions.select(\"prediction\")\n    y_pred = y_pred.toPandas()\n    class_temp = predictions.select(\"label\").groupBy(\"label\")\\\n                        .count().sort('count', ascending=False).toPandas()\n    class_names = class_temp[\"label\"].values.tolist()\n    \n    cm = confusion_matrix(y_true, y_pred, labels = class_names)\n   \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":54},{"cell_type":"code","source":["plot_confusion_matrix(predictions_full)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["We will use to cvModel_gbt as our classifier to classify all the users and make analysis based on the result."],"metadata":{}},{"cell_type":"markdown","source":["#Part 4. Model Application"],"metadata":{}},{"cell_type":"markdown","source":["####Classify the full user data to identify pet owners"],"metadata":{}},{"cell_type":"code","source":["predictions= cvModel_gbt.transform(dataset)\npet_owners = predictions.filter(col('prediction')==1.0)\npet_owners.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n       creator_name|   userid|             comment|               words|            features|       rawPrediction|         probability|prediction|\n+-------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n Think Like A Horse|1411449.0|i love how you al...|[i, love, how, yo...|[0.06695287060990...|[-0.2880773068126...|[0.35981789616864...|       1.0|\nHome Aquatics Hobby|1995671.0|      imma try this!|   [imma, try, this]|[0.15546574940284...|[-0.0101956099006...|[0.49490237168207...|       1.0|\n            Sad Cat|2039444.0|the 2nd dog was t...|[the, 2nd, dog, w...|[0.12009738194935...|[-0.2941174812419...|[0.35703993827053...|       1.0|\n   Kaimuki Backyard| 550340.0|my guppy fries di...|[my, guppy, fries...|[0.03537533077178...|[-0.7689240750038...|[0.17684830820772...|       1.0|\n   Brave Wilderness| 385817.0|*a year later he ...|[a, year, later, ...|[0.08092219396672...|[-0.0734222311071...|[0.46335471022914...|       1.0|\n    VISUALEYESFILMS| 967441.0|exactly luv hit a...|[exactly, luv, hi...|[-0.0230391457676...|[-0.0737382573100...|[0.46319754954185...|       1.0|\n           The Dodo| 473941.0|i found a mouse i...|[i, found, a, mou...|[0.04896848976251...|[-0.7282867922673...|[0.18899194601315...|       1.0|\n       TheMeanKitty|2477782.0|lol!!!¬† oh my wor...|[lol, oh, my, wor...|[-0.0370392208232...|[-0.5307147081293...|[0.25703638616869...|       1.0|\n     Big Cat Rescue|2448356.0|              kitty!|             [kitty]|[0.25328260660171...|[-0.6530059269260...|[0.21315497050281...|       1.0|\n           The Dodo|2469782.0|i have raised sev...|[i, have, raised,...|[0.04547544250936...|[-0.4400376707147...|[0.29316216655448...|       1.0|\n+-------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":59},{"cell_type":"markdown","source":["####Transform the full user data to TF-IDF matrix\nIn order to build a LDA model to analyze the topic of pet owners, we used to tf-idf instead of word2vec to transform the user comments feature, because tf-idf will assign unequal weights to words within documents. <br>\nThe intuition behind it is that if a word occurs multiple times in a document, we should boost its relevance as it should be more meaningful than other words that appear fewer times (TF). At the same time, if a word occurs many times in a document but also along many other documents, maybe it is because this word is just a frequent word; not because it was relevant or meaningful (IDF)."],"metadata":{}},{"cell_type":"code","source":["#Define a function to preprocess the text\n#Remove stopwords\n\nimport re as re\nfrom pyspark.ml.feature import CountVectorizer , IDF\nfrom pyspark.mllib.linalg import Vector, Vectors\nfrom pyspark.mllib.clustering import LDA, LDAModel\nfrom pyspark.ml.feature import StopWordsRemover\n\ndef text_process(df):\n  # Define a list of stop words or use default list\n  remover = StopWordsRemover()\n  Stopwords = remover.getStopWords() \n  remover.setInputCol(\"words\")\n  remover.setOutputCol(\"words_no_stopw\")\n  \n  alpha_udf = udf(lambda tokens: [token for token in tokens if token.isalpha()]) \n  length_udf = udf(lambda tokens: [token for token in tokens if len(token)>2],ArrayType(StringType()))\n  \n  df = remover.transform(df) #remove stop words\n  #df = df.withColumn(\"words_no_stopw\",alpha_udf(\"words_no_stopw\"))   #check if they‚Äôre alpha numeric\n  df = df.withColumn(\"words_no_stopw\",length_udf(\"words_no_stopw\"))  #remove any words or typos which are less than two letters\n  return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":61},{"cell_type":"code","source":["pet_owners = text_process(pet_owners)\n\npet_owners.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+\n       creator_name|   userid|             comment|               words|            features|       rawPrediction|         probability|prediction|      words_no_stopw|\n+-------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+\n Think Like A Horse|1411449.0|i love how you al...|[i, love, how, yo...|[0.06695287060990...|[-0.2880773068126...|[0.35981789616864...|       1.0|[love, always, sa...|\nHome Aquatics Hobby|1995671.0|      imma try this!|   [imma, try, this]|[0.15546574940284...|[-0.0101956099006...|[0.49490237168207...|       1.0|         [imma, try]|\n            Sad Cat|2039444.0|the 2nd dog was t...|[the, 2nd, dog, w...|[0.12009738194935...|[-0.2941174812419...|[0.35703993827053...|       1.0|[2nd, dog, scooby...|\n   Kaimuki Backyard| 550340.0|my guppy fries di...|[my, guppy, fries...|[0.03537533077178...|[-0.7689240750038...|[0.17684830820772...|       1.0|[guppy, fries, di...|\n   Brave Wilderness| 385817.0|*a year later he ...|[a, year, later, ...|[0.08092219396672...|[-0.0734222311071...|[0.46335471022914...|       1.0|[year, later, plate]|\n    VISUALEYESFILMS| 967441.0|exactly luv hit a...|[exactly, luv, hi...|[-0.0230391457676...|[-0.0737382573100...|[0.46319754954185...|       1.0|[exactly, luv, hi...|\n           The Dodo| 473941.0|i found a mouse i...|[i, found, a, mou...|[0.04896848976251...|[-0.7282867922673...|[0.18899194601315...|       1.0|[found, mouse, ba...|\n       TheMeanKitty|2477782.0|lol!!!¬† oh my wor...|[lol, oh, my, wor...|[-0.0370392208232...|[-0.5307147081293...|[0.25703638616869...|       1.0|[lol, word, cute,...|\n     Big Cat Rescue|2448356.0|              kitty!|             [kitty]|[0.25328260660171...|[-0.6530059269260...|[0.21315497050281...|       1.0|             [kitty]|\n           The Dodo|2469782.0|i have raised sev...|[i, have, raised,...|[0.04547544250936...|[-0.4400376707147...|[0.29316216655448...|       1.0|[raised, several,...|\n+-------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":62},{"cell_type":"markdown","source":["Transform the daraCountVectorizer takes this data and returns a sparse matrix of term frequencies attached to the original Dataframe. Same thing goes for the IDF."],"metadata":{}},{"cell_type":"code","source":["pet_owners = pet_owners.select(\"words_no_stopw\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":64},{"cell_type":"code","source":["# TF\ncv = CountVectorizer(inputCol=\"words_no_stopw\", \n                     outputCol=\"raw_features\",  \n                     vocabSize = 5000,\n                     minDF=5.0) # the minimum number (or fraction if < 1.0) of documents a term must appear in to be included in the vocabulary.\n\ncvmodel = cv.fit(pet_owners)\nresult_cv = cvmodel.transform(pet_owners)\nresult_cv.show()\nprint(len(cvmodel.vocabulary))  # vocabulary size "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+\n      words_no_stopw|        raw_features|\n+--------------------+--------------------+\n[love, always, sa...|(5000,[2,19,32,73...|\n         [imma, try]|(5000,[137,2154],...|\n[2nd, dog, scooby...|(5000,[1,28,161,2...|\n[guppy, fries, di...|(5000,[67,87,227,...|\n[year, later, plate]|(5000,[75,345,212...|\n[exactly, luv, hi...|(5000,[323,393,22...|\n[found, mouse, ba...|(5000,[8,31,47,59...|\n[lol, word, cute,...|(5000,[2,19,25,35...|\n             [kitty]|  (5000,[142],[1.0])|\n[raised, several,...|(5000,[5,38,67,87...|\n[rainy, gloomy, d...|(5000,[0,1,29,33,...|\n[thumbnail, thoug...|(5000,[5,19,26,11...|\n[bruh, ended, fck...|(5000,[846,2132],...|\n[story, continues...|(5000,[3,19,33,49...|\n[love, dogs, fine...|(5000,[2,14,344,8...|\n[omg, got, new, p...|(5000,[8,33,76,82...|\n[catnip, basicall...|(5000,[10,792,970...|\n[mansplaining, la...|(5000,[13,67,418]...|\n[wow, sister, sai...|(5000,[5,12,18,24...|\n[taxonomist, hopp...|(5000,[63,74,87,1...|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n5000\n</div>"]}}],"execution_count":65},{"cell_type":"code","source":["# IDF\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\nidfModel = idf.fit(result_cv)\nresult_tfidf = idfModel.transform(result_cv) \nresult_tfidf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+--------------------+\n      words_no_stopw|        raw_features|            features|\n+--------------------+--------------------+--------------------+\n[love, always, sa...|(5000,[2,19,32,73...|(5000,[2,19,32,73...|\n         [imma, try]|(5000,[137,2154],...|(5000,[137,2154],...|\n[2nd, dog, scooby...|(5000,[1,28,161,2...|(5000,[1,28,161,2...|\n[guppy, fries, di...|(5000,[67,87,227,...|(5000,[67,87,227,...|\n[year, later, plate]|(5000,[75,345,212...|(5000,[75,345,212...|\n[exactly, luv, hi...|(5000,[323,393,22...|(5000,[323,393,22...|\n[found, mouse, ba...|(5000,[8,31,47,59...|(5000,[8,31,47,59...|\n[lol, word, cute,...|(5000,[2,19,25,35...|(5000,[2,19,25,35...|\n             [kitty]|  (5000,[142],[1.0])|(5000,[142],[4.30...|\n[raised, several,...|(5000,[5,38,67,87...|(5000,[5,38,67,87...|\n[rainy, gloomy, d...|(5000,[0,1,29,33,...|(5000,[0,1,29,33,...|\n[thumbnail, thoug...|(5000,[5,19,26,11...|(5000,[5,19,26,11...|\n[bruh, ended, fck...|(5000,[846,2132],...|(5000,[846,2132],...|\n[story, continues...|(5000,[3,19,33,49...|(5000,[3,19,33,49...|\n[love, dogs, fine...|(5000,[2,14,344,8...|(5000,[2,14,344,8...|\n[omg, got, new, p...|(5000,[8,33,76,82...|(5000,[8,33,76,82...|\n[catnip, basicall...|(5000,[10,792,970...|(5000,[10,792,970...|\n[mansplaining, la...|(5000,[13,67,418]...|(5000,[13,67,418]...|\n[wow, sister, sai...|(5000,[5,12,18,24...|(5000,[5,12,18,24...|\n[taxonomist, hopp...|(5000,[63,74,87,1...|(5000,[63,74,87,1...|\n+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":66},{"cell_type":"markdown","source":["####Build a LDA model - topic modeling"],"metadata":{}},{"cell_type":"markdown","source":["##### Why LDA?\nLDA trades off two ‚Äúconflicting‚Äù goals: <br>\n1: For each document, allocate its words to as few topics aspossible. <br>\n2: For each topic, assign high probability to as few terms aspossible. <br>\nLDA is a probabilistic model of text used to find topics thatdescribe a corpus.<br>\nLDA casts the problem of discovering themes in largedocument collections as a posterior inference problem.LDA lets us visualize the hidden thematic structure in largecollections, and generalize new data to fit into that structure."],"metadata":{}},{"cell_type":"code","source":["\nfrom pyspark.ml.clustering import LDA\nnum_topics = 8\nlda = LDA(k = num_topics,\n          maxIter = 50 #number of iterations\n          )\n\nldaModel = lda.fit(result_tfidf)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":69},{"cell_type":"markdown","source":["#####Print the top 20 words in each topic"],"metadata":{}},{"cell_type":"code","source":["# Print topics and top-weighted terms\ntopics = ldaModel.describeTopics(maxTermsPerTopic=20) #return the index of words from dictionary created by CountVector\nvocabArray = cvmodel.vocabulary\n\nListOfIndexToWords = udf(lambda wl: list([vocabArray[w] for w in wl]))\n#FormatNumbers = udf(lambda nl: [\"{:1.4f}\".format(x) for x in nl])\n\ntopics_words = topics.select(ListOfIndexToWords(topics.termIndices).alias('words'))\ntopics_words.show(truncate=False, n=num_topics)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------------------------------------------------------------------------------------------------+\nwords                                                                                                                                       |\n+--------------------------------------------------------------------------------------------------------------------------------------------+\n[cry, one, crying, like, cat, dog, made, little, dude, got, lol, love, aww, lost, back, haha, hungry, really, time, weird]                  |\n[kitty, fish, black, like, one, something, know, tank, cat, dont, heart, think, dog, someone, leg, well, get, got, done, time]              |\n[love, channel, videos, video, animals, happy, much, one, like, watching, really, dog, get, year, great, day, birthday, always, watch, well]|\n[cats, cat, like, looks, kittens, cute, lol, kitten, love, hair, adorable, one, bird, food, want, dog, black, robin, get, adopt]            |\n[dog, husky, dogs, like, puppy, omg, rabbit, gohan, cute, love, cried, want, max, bunny, wants, wish, mom, get, shelby, one]                |\n[male, female, like, dog, house, bed, sleep, one, dont, time, also, know, get, dogs, got, around, run, birds, never, put]                   |\n[horse, dog, horses, german, like, get, hamster, got, dont, bit, ride, riding, said, shepherd, back, think, fuck, kid, never, one]          |\n[people, know, dont, help, mother, get, care, like, animals, one, anyone, didnt, vet, want, dogs, else, love, really, dog, puppies]         |\n+--------------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":71},{"cell_type":"markdown","source":["#####Visualize the wordcloud of each topic"],"metadata":{}},{"cell_type":"code","source":["!pip install wordcloud\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n#Define a function to draw wordclouds of topics\n\ndef wordcloud(topics):\n  wordcloud = []\n  for i in range(len(topics)):\n    text = str(topics.iloc[i,0])\n    #text = \" \".join([(k + \" \")*v for k,v in topics.iloc[i].items()])\n    #temp_cloud = WordCloud().generate(text)\n    temp_cloud = WordCloud(background_color=\"white\", max_words=10000, collocations = False,\n               contour_width=3, contour_color='steelblue',max_font_size=40)\n    temp_cloud = temp_cloud.generate(text) # Generate a word cloud image\n    if len(wordcloud) == 0:\n      wordcloud = [temp_cloud]\n    else:\n      wordcloud.append(temp_cloud)\n  # Display the generated image:\n  # the matplotlib way:\n  w=10\n  h=10\n  fig=plt.figure(figsize=(40,10))\n  columns = 4\n  rows = 2\n  for i in range(1, columns*rows +1):\n      img = wordcloud [i-1]\n      fig.add_subplot(rows, columns, i)\n      plt.imshow(img)\n      plt.axis(\"off\")    \n  plt.show()\n  \n "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":73},{"cell_type":"code","source":["wordcloud(topics_pdf)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["#### 4. Identify Creators With Cat And Dog Owners In The Audience"],"metadata":{}},{"cell_type":"code","source":["#Identify cat and dog owners\npredictions= cvModel_gbt.transform(dataset)\npet_owner= predictions.filter(col('prediction')==1.0).select('creator_name','userid','comment')\npet_owner.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\ncreator_name       |userid   |comment                                                                                                                                                                                                                                                                                                                                                                         |\n+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nThink Like A Horse |1411449.0|i love how you always say dummys or shut up lol makes me laugh                                                                                                                                                                                                                                                                                                                  |\nHome Aquatics Hobby|1995671.0|imma try this!                                                                                                                                                                                                                                                                                                                                                                  |\nSad Cat            |2039444.0|the 2nd dog was the scooby doo breed. id never seen it before.                                                                                                                                                                                                                                                                                                                  |\nKaimuki Backyard   |550340.0 |my guppy fries died 1 or 2 every week is it normal?                                                                                                                                                                                                                                                                                                                             |\nBrave Wilderness   |385817.0 |*a year later he is now on a plate. . . *                                                                                                                                                                                                                                                                                                                                       |\nVISUALEYESFILMS    |967441.0 |exactly luv hit after hit                                                                                                                                                                                                                                                                                                                                                       |\nThe Dodo           |473941.0 |i found a mouse in my basement once that was stuck to a sticky spider trap. i didnt have the heart to leave him there to die so i got my garden gloves and took the whole thing outside and got him unstuck which took some doing but he was ok. then i let him go over the fence in a field.                                                                                   |\nTheMeanKitty       |2477782.0|lol!!!¬† oh my word that is too cute &amp; so precious!¬† i love her reaction!¬† her mind was totally blown!                                                                                                                                                                                                                                                                           |\nBig Cat Rescue     |2448356.0|kitty!                                                                                                                                                                                                                                                                                                                                                                          |\nThe Dodo           |2469782.0|i have raised several squirrels.  they are 1 person animals.  i was devastated when they died- the oldest being 5 1/2. they are lovable intelligent  and you get so attached.  mine slept with me every night  &amp; went to work with my husband.                                                                                                                                  |\nGohan The Husky    |1416861.0|on a rainy  gloomy day found this wonderful channel..specially because gohan looks like mishka ..... reminds me of her... seriously guy u have a wonderful dog. keep making vedios for us                                                                                                                                                                                       |\nSolid Gold Aquatics|1672859.0|the thumbnail though lol also how did they get into your aquarium?                                                                                                                                                                                                                                                                                                              |\nChriskillergaming  |2485163.0|bruh you ended my fcking streak üò≠                                                                                                                                                                                                                                                                                                                                              |\nBrave Wilderness   |2044294.0|the story continues....shortly after they have released the velvet worm.....worm: folks! there is something - there was this giant eye watching me!worms: lol!worm: seriously! it was a giant life form and it took me away!worms: sure and did they anal probe you? lmao!worm: bleh! you guys suck. one day youll realize that im right.worms: take less drugs and do you work.|\nBrian Barczyk      |233974.0 |i love dogs a i am fine you showing your emissions                                                                                                                                                                                                                                                                                                                              |\nstacyvlogs         |24582.0  |omg we got my new puppy ruby and she had kennel cofth the day be for we wint on vachion but we got her and she is the best                                                                                                                                                                                                                                                      |\nCole &amp; Marmalade   |334337.0 |catnip is basically weed for cats then...                                                                                                                                                                                                                                                                                                                                       |\nPaper Ghost        |573364.0 |the mansplaining lady triggers me every time. every time.                                                                                                                                                                                                                                                                                                                       |\nBrave Wilderness   |1583886.0|wow my sister said i want to see him get bit! (she is only 4 years old) :)                                                                                                                                                                                                                                                                                                      |\nHoppingHammy       |158503.0 |taxonomist and hoppinghammy you saved my hamsters life!! thank you!! he would have died if i couldnt have done this until the emergency vet arrived!                                                                                                                                                                                                                            |\n+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":76},{"cell_type":"code","source":["#Compute the fraction of cat or dog owners\nowner_fr = round(pet_owner.count()/predictions.count()*100,2)\nprint(str(owner_fr)+ '% of the total users own cats or dogs in this Youtube comment dataset.')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">10.74% of the total users own cats or dogs in this Youtube comment dataset.\n</div>"]}}],"execution_count":77},{"cell_type":"markdown","source":["#### 5. Analysis and Future work"],"metadata":{}},{"cell_type":"markdown","source":["Step 1: Identify Cat And Dog Owners\nFind the users who are cat and/or dog owners.\n\nStep 2: Build And Evaluate Classifiers\nBuild classifiers for the cat and dog owners and measure the performance of the classifiers.\n\nStep 3: Classify All The Users\nApply the cat/dog classifiers to all the users in the dataset. Estimate the fraction of all users\nwho are cat/dog owners.\n\nStep 4: Extract Insights About Cat And Dog Owners\nFind topics important to cat and dog owners.\n\nStep 5: Identify Creators With Cat And Dog Owners In The Audience\nFind creators with the most cat and/or dog owners. Find creators with the highest statistically\nsignificant percentages of cat and/or dog owners."],"metadata":{}},{"cell_type":"markdown","source":["#Report\n**Motivation:** My motivation of this project is because I myself own a cat, and I love watching cat related videos on Youtube. As I watch more and more videos, Youtube's algorithm was able to learn my video-watching habit and started to recommend more related content to me. But I feel like by identifying me as a cat owner, there can be more cool stuff to be done which is beyond the video recommendations. <br>\n\n**Data and Method:** In this project, I used a sample of the Youtube comments text data to build a classifier to predict the pet and dog owners. Then, using the model trained, I identified those cat and dog owners in the full dataset. Then I built a LDA (Latent Dirichlet Allocation) model to classify the user's comments into 8 topics and plotted the wordcloud of each topic. <br>\nFor example, after recognizing the cat and dog owners, we can do in-depth analysis on these user's comments, then there will be changces that we gain valuable insights from those comments and find potential hot topics among this cohort. These findings can be applied to product design for pet services/product companies or pet video creators, because they can gain a good sense of what's been heavily discussed and what's trending right now, so that they can tailor their products/content to better target the specific group of users. <br>\n#####Let's walk through how this project's been done!\n**Step 1: Data Exploration and Cleaning**<br>\n(1)There are 5,820,035 rows of user comments in total.Rows with null user id have been kept but those will null comments have been dropped. Comments are been converted to lowercase.<br> \n(2)Selected 2,000,000 rows of data to implement this whole project considering the time complexity. (Probably will run with the whole dataset again in the future)<br> \n(3)Tokenized the comment and created word2Vec features in order to train a classification model. <br> \n(4)Randomly sampled 1,000,000 rows of data to train the classifier and manually labeled the comments as 'has cat&dog' and 'no cat&dog'.<br> \n(5)This is a highly imbalanced dataset (with negative labels around 100 times more than positive labels), so I downsampled the negative labels (1 for positive:2 for negative).\n\n**Step 2: Model Selection and Tuning**<br> \n(1)Splitted the data into training set and testing set.<br>\n(2)Tained a classifier using three types of machine learning algorithm: logistic regression, random forest, and Gradient Boosting.<br>\n(3)Utilized ROC/AUC, confusion matrix, accuracy, precision and recall as model evaluation metrics to evaluate the three models performance.<br>\n(4)Selected Gradient Boosted algorithm to do hyparameter tuning since it has the best performance on the test data set.<br>\n(5)Tuned the hyperparameters based on grid search and re-evaluated the model performace: Accuracy (0.87), Precision (0.8), Recall (0.82), AUV of ROC (0.94).\n\n**Step 3: Model Application - LDA**<br>\n(1)Applied the best GDBT model onto the full dataset and identified users who own dogs or cats. 10.74% of users own dogs or cats. <br>\n(2)Preprocessed the dog&cat owners' text data to TF-IDF matrices and built a LDA model - set the topic number to 8. (This can be further improved on)<br>\n(3)Got the LDA topic result and visualized the wordcloud of each topic based on word frequencied of each topic.\n\n**Step 4: Insights gained from topic modeling**<br>\n(1)Based on the wordcloud, the topic of each segment is not that easy to interpret, but I will try my best to garner as much information as possible based on my business sense.<br>\n(2)Semantic meaning of each topic: Cat/Dog crying, kitty eating fish, I love waching this dog/animal Youtube channel, Cats are cute/adorable (chasing bird), Dog(husky) are cute (chasing rabbit?), gender differences for dogs, hourse related content, People care for dogs/animals.\n\n**Recommendation based on the insights:**<br>\n(1)Dog/cat related Youtube channels are super popular among dog/cat owners, this descripes their watching habit and Youtubers can post more videos to attract these users.<br>\n(2)People love watching kitten eating fish/chasing bird, dogs (husky in particular) chaing rabbit, and content related to hourse riding. Youtubers can incorporate those content into their videos. <br>\n(3)Following up on the above mentioned topics, ads for dog/cat competitions/social events can target this cohort, and horse-riding activity companies can see this cohort as their potential customers as well, since people watching those videos may develop an intention to engage in those activities in real life.<br>\n(4)People are having discussions around pet genders and carings, it suggests that people with dog&cat pay more attention to animal care. Ads about pet commutiny (donation, adoption) can target those users.\n\n**Area of Improvements of this project:**<br>\n(1)Use the whole dataset may give better results in both models.<br>\n(2)Find a better way to label the comment - this can be done by researching more into the pet owner behavior.<br>\n(3)Get more user feature data other than user id, such as age, location, occupation, family size and some other behavioral features like Youtube watching record. - Based on those data, we can do user segmentation analysis before and after the classification as to get better results of identifying users and analyzing the owners' preferences.\n(4)For the LDA part, we can calculate the model perplxity to select the optimal number of topic. We can also do more feature engineering to make the text data more ingestible."],"metadata":{}}],"metadata":{"name":"Youtube text data analysis","notebookId":1425635057729673},"nbformat":4,"nbformat_minor":0}
